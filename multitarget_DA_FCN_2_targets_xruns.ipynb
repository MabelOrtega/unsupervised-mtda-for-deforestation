{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1643838967217,
     "user": {
      "displayName": "Mabel Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiEAglk2m-Q23ZrM2KNCJoD9UwEO0qVuRoT_b7RmQ8=s64",
      "userId": "05993491897406658405"
     },
     "user_tz": 300
    },
    "id": "DCSKQER6BViH",
    "outputId": "ad18fe1a-ce4e-4330-d99f-b6346d6e72b3"
   },
   "outputs": [],
   "source": [
    "from utils_mt import *\n",
    "from networks import *\n",
    "root_path = os.getcwd()\n",
    "print(root_path)\n",
    "\n",
    "# folder to load config file\n",
    "CONFIG_PATH = root_path\n",
    "config = load_config(CONFIG_PATH, 'main_config.yaml')\n",
    "#print('config: ', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 0\n",
    "path_exp = root_path+'/avg_experiments/exp'+str(exp)\n",
    "path_models = path_exp+'/models'\n",
    "path_maps = path_exp+'/pred_maps'\n",
    "\n",
    "if not os.path.exists(path_exp):\n",
    "    os.makedirs(path_exp)   \n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)   \n",
    "if not os.path.exists(path_maps):\n",
    "    os.makedirs(path_maps)\n",
    "\n",
    "dir_data = config['data_directory']\n",
    "tr_folder = 'train'\n",
    "vl_folder = 'val'\n",
    "channels = 20 # each image \n",
    "num_classes = 3\n",
    "\n",
    "# list of folders to be merged\n",
    "# SOURCE - TARGET1 - TARGET2\n",
    "list_dir = ['MT_1C', 'PA_1C', 'MA_1C']\n",
    "test_name = 'PA'\n",
    "patches_tg = 'patches_certain'\n",
    "\n",
    "save_prob = False\n",
    "times = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1643838972348,
     "user": {
      "displayName": "Mabel Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiEAglk2m-Q23ZrM2KNCJoD9UwEO0qVuRoT_b7RmQ8=s64",
      "userId": "05993491897406658405"
     },
     "user_tz": 300
    },
    "id": "id2MkWU2BHK6",
    "outputId": "9f5a39cf-97d9-452b-82ac-d64922d63c71"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "patch_size = config['patch_size']\n",
    "latent_dim = config['channels']\n",
    "cdims = latent_dim//2\n",
    "num_clases = config['num_classes']\n",
    "\n",
    "lambda_r = config['lambda_r_']\n",
    "lambda_ds = config['lambda_ds_']\n",
    "lambda_de = config['lambda_de_']\n",
    "lambda_c = config['lambda_c_']\n",
    "\n",
    "print('Parameters')\n",
    "print('\\n', 'lr :', lr, '\\n', 'batch_size: ', batch_size, '\\n', 'patch_size: ', patch_size, '\\n', 'num_clases: ', num_clases)\n",
    "\n",
    "print('\\n', 'lambda r :', lambda_r, '\\n', 'lambda_ds: ', lambda_ds, '\\n', 'lambda_de: ', lambda_de, '\\n', 'lambda_c: ', lambda_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computation graph\n",
    "K.clear_session()\n",
    "\n",
    "# Inputs\n",
    "img_sr = K.placeholder(dtype=tf.float32, name=\"img_source\", shape=(batch_size, patch_size, patch_size, cdims))\n",
    "img_tr1 = K.placeholder(dtype=tf.float32, name=\"img_target1\", shape=(batch_size, patch_size, patch_size, cdims))\n",
    "img_tr2 = K.placeholder(dtype=tf.float32, name=\"img_target2\", shape=(batch_size, patch_size, patch_size, cdims))\n",
    "domain_label_sr = K.placeholder(dtype=tf.float32, name='source_domain_label', shape= (batch_size, 3))\n",
    "domain_label_tr1 = K.placeholder(dtype=tf.float32, name='target1_domain_label', shape= (batch_size, 3))\n",
    "domain_label_tr2 = K.placeholder(dtype=tf.float32, name='target2_domain_label', shape= (batch_size, 3))\n",
    "class_label_sr = K.placeholder(dtype=tf.float32, name='source_mask_label',\n",
    "                               shape= (batch_size, patch_size, patch_size, num_clases))\n",
    "\n",
    "# encoders definition\n",
    "nb_filters_sh = 16\n",
    "encoder_sh = build_encoder_sh((patch_size, patch_size, cdims), nb_filters=nb_filters_sh, name='enc_sh_')\n",
    "encoder_ex = build_encoder_ex((patch_size, patch_size, cdims), nb_filters=nb_filters_sh, name='enc_ex_')\n",
    "\n",
    "encoder_sh.summary()\n",
    "encoder_ex.summary()\n",
    "\n",
    "# Getting output feats shapes\n",
    "sh_out_shape = encoder_sh.layers[-1].output_shape[1:]\n",
    "ex_out_shape = sh_out_shape\n",
    "\n",
    "# decoder definition\n",
    "decoder_sh = build_decoder(sh_out_shape, ex_out_shape, out_dim = cdims, nb_filters=16, name='dec_')\n",
    "decoder_sh.summary()\n",
    "\n",
    "# discriminator definition\n",
    "domain_discriminator = build_discriminator((sh_out_shape), nb_filters=4, num_domains = 3, name='dis_')\n",
    "domain_discriminator.summary()\n",
    "\n",
    "# classifier definition\n",
    "nb_filters_c = 16\n",
    "classifier = build_classifier((sh_out_shape), nb_filters=nb_filters_c, num_classes = 3, name='cl_')\n",
    "classifier.summary()\n",
    "\n",
    "# features shared\n",
    "feats_sh_sr = encoder_sh(img_sr)\n",
    "feats_sh_tr1 = encoder_sh(img_tr1)\n",
    "feats_sh_tr2 = encoder_sh(img_tr2)\n",
    "\n",
    "# features exclusive\n",
    "feats_ex_sr = encoder_ex(img_sr)\n",
    "feats_ex_tr1 = encoder_ex(img_tr1)\n",
    "feats_ex_tr2 = encoder_ex(img_tr2)\n",
    "\n",
    "# decoder\n",
    "rec_sr = decoder_sh([feats_sh_sr, feats_ex_sr])\n",
    "rec_tr1 = decoder_sh([feats_sh_tr1, feats_ex_tr1])\n",
    "rec_tr2 = decoder_sh([feats_sh_tr2, feats_ex_tr2])\n",
    "\n",
    "# domain discriminator\n",
    "dom_sh_sr_logits, _ = domain_discriminator(feats_sh_sr)\n",
    "dom_ex_sr_logits, _ = domain_discriminator(feats_ex_sr)\n",
    "\n",
    "# target 1\n",
    "dom_sh_tr1_logits, _ = domain_discriminator(feats_sh_tr1)\n",
    "dom_ex_tr1_logits, _ = domain_discriminator(feats_ex_tr1)\n",
    "\n",
    "# target 2\n",
    "dom_sh_tr2_logits, _ = domain_discriminator(feats_sh_tr2)\n",
    "dom_ex_tr2_logits, _ = domain_discriminator(feats_ex_tr2)\n",
    "\n",
    "# classifier\n",
    "cl_sr_logits, _ = classifier(feats_sh_sr)\n",
    "cl_tr1_logits, _ = classifier(feats_sh_tr1)\n",
    "cl_tr2_logits, _ = classifier(feats_sh_tr2)\n",
    "\n",
    "# Computing loss\n",
    "\n",
    "# loss decoder\n",
    "loss_dec = lambda_r * (tf.reduce_mean(tf.abs(img_sr-rec_sr)) + tf.reduce_mean(tf.abs(img_tr1-rec_tr1)) + \\\n",
    "                      tf.reduce_mean(tf.abs(img_tr2-rec_tr2))) \n",
    "\n",
    "# loss domain classifier\n",
    "\n",
    "loss_feat_ex_dom_dis = lambda_de * (tf.reduce_mean(softmax_loss_d(domain_label_sr, dom_ex_sr_logits)) + \\\n",
    "                                   tf.reduce_mean(softmax_loss_d(domain_label_tr1, dom_ex_tr1_logits)) + \\\n",
    "                                   tf.reduce_mean(softmax_loss_d(domain_label_tr2, dom_ex_tr2_logits)))\n",
    "\n",
    "loss_feat_sh_dom_dis = lambda_ds * (tf.reduce_mean(softmax_loss_d(domain_label_sr, dom_sh_sr_logits)) + \\\n",
    "                                   tf.reduce_mean(softmax_loss_d(domain_label_tr1, dom_sh_tr1_logits))+ \\\n",
    "                                   tf.reduce_mean(softmax_loss_d(domain_label_tr2, dom_sh_tr2_logits)))\n",
    "\n",
    "loss_dis = loss_feat_sh_dom_dis + loss_feat_ex_dom_dis\n",
    "\n",
    "# Regularization term\n",
    "#cl_reg = lambda_c * entropy_criterion(cl_tr_logits)\n",
    "\n",
    "# loss classifier label\n",
    "loss_cl = tf.reduce_mean(softmax_loss_c(class_label_sr, cl_sr_logits)) #+ cl_reg\n",
    "\n",
    "# loss classifier share\n",
    "loss_sh_cl = lambda_c * tf.reduce_mean(softmax_loss_c(class_label_sr, cl_sr_logits)) #+ cl_reg\n",
    "\n",
    "# loss shared encoder\n",
    "loss_sh = loss_dec + loss_sh_cl - loss_feat_sh_dom_dis \n",
    "\n",
    "# loss exclusive encoder\n",
    "loss_ex = loss_dec + loss_feat_ex_dom_dis\n",
    "\n",
    "# Collecting variables for training\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "e_sh_vars = [var for var in t_vars if 'enc_sh_' in var.name] # Encoder shared variables\n",
    "e_ex_vars = [var for var in t_vars if 'enc_ex_' in var.name] # Encoder exclusive variables\n",
    "cl_vars =   [var for var in t_vars if 'cl_' in var.name] # Classifier variables\n",
    "dec_vars =  [var for var in t_vars if 'dec_' in var.name] # Decoder variables\n",
    "dis_vars =  [var for var in t_vars if 'dis_' in var.name] # Discriminator variables\n",
    "\n",
    "# Optimizer parameters\n",
    "lr_d = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "# Assings variables and corresponding lossses to be minimized\n",
    "sh_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(loss_sh, var_list = e_sh_vars )\n",
    "ex_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(loss_ex, var_list = e_ex_vars)\n",
    "cl_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(loss_cl, var_list = cl_vars)\n",
    "dec_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(loss_dec, var_list = dec_vars)\n",
    "dis_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(loss_dis, var_list = dis_vars)\n",
    "\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_models +'/'+'enc_sh_model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        encoder_sh.summary()\n",
    "\n",
    "with open(path_models +'/'+'enc_ex_model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        encoder_ex.summary()\n",
    "        \n",
    "with open(path_models +'/'+'dec_model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        decoder_sh.summary()\n",
    "\n",
    "with open(path_models +'/'+'dis_model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        domain_discriminator.summary()\n",
    "        \n",
    "with open(path_models +'/'+'cl_model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1643838974887,
     "user": {
      "displayName": "Mabel Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiEAglk2m-Q23ZrM2KNCJoD9UwEO0qVuRoT_b7RmQ8=s64",
      "userId": "05993491897406658405"
     },
     "user_tz": 300
    },
    "id": "3G865CemBHK7",
    "outputId": "1d876102-1901-40e7-a04a-44014a85272a"
   },
   "outputs": [],
   "source": [
    "train_gen = data_gen(dir_data, list_dir, tr_folder, batch_size, patch_size, channels, num_classes, patches_tg)\n",
    "tr_samples, _ = retrieve_num_samples(dir_data, list_dir, tr_folder, patches_tg)\n",
    "print('Total training samples sr: ', len(tr_samples[0]))\n",
    "print('Total training samples tr1: ', len(tr_samples[1]))\n",
    "print('Total training samples tr2: ', len(tr_samples[2]))\n",
    "tr_samples_min = min(len(tr_samples[0]), len(tr_samples[1]), len(tr_samples[2]))\n",
    "print(tr_samples_min)\n",
    "\n",
    "valid_gen = data_gen(dir_data, list_dir, vl_folder, batch_size, patch_size, channels, num_clases, patches_tg)\n",
    "vl_samples,_ = retrieve_num_samples(dir_data, list_dir, vl_folder, patches_tg)\n",
    "print('Total validation samples sr: ', len(vl_samples[0]))\n",
    "print('Total validation samples tr1: ', len(vl_samples[1]))\n",
    "print('Total validation samples tr2: ', len(vl_samples[2]))\n",
    "vl_samples_min = min(len(vl_samples[0]), len(vl_samples[1]), len(vl_samples[2]))\n",
    "print(vl_samples_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_time =[]\n",
    "for tm in range(0,times):\n",
    "    print('time: ', tm)\n",
    "    \n",
    "    path_model_tm = path_models + '/run_'+str(tm)\n",
    "    path_loss_tm = path_exp +'/loss/'+ '/run_'+str(tm)\n",
    "    if not os.path.exists(path_model_tm):\n",
    "        os.makedirs(path_model_tm) \n",
    "    if not os.path.exists(path_loss_tm):\n",
    "        os.makedirs(path_loss_tm) \n",
    "\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    index_plt = np.random.randint(batch_size)\n",
    "\n",
    "    num_of_trn_batches = tr_samples_min // batch_size\n",
    "    num_of_val_batches = vl_samples_min // batch_size\n",
    "    epochs = 200\n",
    "    best_val_loss = np.inf\n",
    "    patience = 10\n",
    "    tr_dec, tr_dis, tr_esh, tr_eex, tr_cls, tr_dsh, tr_dex = [], [], [], [], [], [], []\n",
    "    vl_dec, vl_dis, vl_esh, vl_eex, vl_cls, vl_dsh, vl_dex = [], [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch :', epoch)\n",
    "        enc_sh_loss, enc_ex_loss, dec_loss, dis_loss, ex_dis_loss, sh_dis_loss, cl_loss = [], [], [], [], [], [], []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for idx in range(0, num_of_trn_batches):\n",
    "\n",
    "            # selecting a batch of images\n",
    "            batch_img, batch_mask, batch_dom = next(train_gen)\n",
    "            batch_img_sr = batch_img[:,:,:,:,0]\n",
    "            batch_img_tr1 = batch_img[:,:,:,:,1]\n",
    "            batch_img_tr2 = batch_img[:,:,:,:,2]\n",
    "            batch_label_sr = batch_mask[:,:,:,:,0]\n",
    "            batch_label_tr1 = batch_mask[:,:,:,:,1]\n",
    "            batch_label_tr2 = batch_mask[:,:,:,:,2]\n",
    "            batch_dom_sr = batch_dom[:,:,0]\n",
    "            batch_dom_tr1 = batch_dom[:,:,1]\n",
    "            batch_dom_tr2 = batch_dom[:,:,2]\n",
    "\n",
    "            if batch_img_sr.shape[0] != batch_size or batch_img_tr1.shape[0] != batch_size or batch_img_tr2.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            feed_dict = {img_sr: batch_img_sr, img_tr1: batch_img_tr1, img_tr2: batch_img_tr2,\n",
    "                        domain_label_sr: batch_dom_sr, domain_label_tr1:batch_dom_tr1, domain_label_tr2:batch_dom_tr2,\n",
    "                        class_label_sr: batch_label_sr}\n",
    "\n",
    "            sess.run([sh_optim], feed_dict=feed_dict)\n",
    "            sess.run([ex_optim], feed_dict=feed_dict)\n",
    "            sess.run([cl_optim], feed_dict=feed_dict)\n",
    "            sess.run([dec_optim], feed_dict=feed_dict)\n",
    "            sess.run([dis_optim], feed_dict=feed_dict)\n",
    "\n",
    "            with sess.as_default():\n",
    "                enc_sh_loss_ = loss_sh.eval(feed_dict)\n",
    "                enc_sh_loss.append(enc_sh_loss_)\n",
    "                enc_ex_loss_ = loss_ex.eval(feed_dict)\n",
    "                enc_ex_loss.append(enc_ex_loss_)\n",
    "                cl_loss_ = loss_cl.eval(feed_dict)\n",
    "                cl_loss.append(cl_loss_)\n",
    "                dec_loss_ = loss_dec.eval(feed_dict)\n",
    "                dec_loss.append(dec_loss_)\n",
    "                dis_loss_ = loss_dis.eval(feed_dict)\n",
    "                dis_loss.append(dis_loss_)\n",
    "                loss_ex_dis_ = loss_feat_ex_dom_dis.eval(feed_dict)\n",
    "                ex_dis_loss.append(loss_ex_dis_)\n",
    "                loss_sh_dis_ = loss_feat_sh_dom_dis.eval(feed_dict)\n",
    "                sh_dis_loss.append(loss_sh_dis_)\n",
    "\n",
    "            if idx % num_of_trn_batches == 0:\n",
    "                feats_sh_sr_ = encoder_sh.predict(batch_img_sr)\n",
    "                feats_sh_tr1_ = encoder_sh.predict(batch_img_tr1)\n",
    "                feats_sh_tr2_ = encoder_sh.predict(batch_img_tr2)\n",
    "\n",
    "                feats_ex_sr_ = encoder_ex.predict(batch_img_sr)\n",
    "                feats_ex_tr1_ = encoder_ex.predict(batch_img_tr1)\n",
    "                feats_ex_tr2_ = encoder_ex.predict(batch_img_tr2)\n",
    "\n",
    "                rec_sr_ = decoder_sh.predict([feats_sh_sr_, feats_ex_sr_])\n",
    "                rec_tr1_ = decoder_sh.predict([feats_sh_tr1_, feats_ex_tr1_])\n",
    "                rec_tr2_ = decoder_sh.predict([feats_sh_tr2_, feats_ex_tr2_])\n",
    "\n",
    "                _, dom_sh_sr_ = domain_discriminator.predict(feats_sh_sr_)\n",
    "                _, dom_ex_sr_ = domain_discriminator.predict(feats_ex_sr_)\n",
    "\n",
    "                _, dom_sh_tr1_ = domain_discriminator.predict(feats_sh_tr1_)\n",
    "                _, dom_ex_tr1_ = domain_discriminator.predict(feats_ex_tr1_)\n",
    "\n",
    "                _, dom_sh_tr2_ = domain_discriminator.predict(feats_sh_tr2_)\n",
    "                _, dom_ex_tr2_ = domain_discriminator.predict(feats_ex_tr2_)\n",
    "\n",
    "                _, cl_sr_ = classifier.predict(feats_sh_sr_)\n",
    "                _, cl_tr1_ = classifier.predict(feats_sh_tr1_)\n",
    "                _, cl_tr2_ = classifier.predict(feats_sh_tr2_)\n",
    "\n",
    "                true_sr = batch_label_sr[index_plt].argmax(axis=-1) \n",
    "                pred_sr = cl_sr_[index_plt].argmax(axis=-1)\n",
    "                true_tr1 = batch_label_tr1[index_plt].argmax(axis=-1)\n",
    "                pred_tr1 = cl_tr1_[index_plt].argmax(axis=-1)\n",
    "                true_tr2 = batch_label_tr2[index_plt].argmax(axis=-1)\n",
    "                pred_tr2 = cl_tr2_[index_plt].argmax(axis=-1)\n",
    "\n",
    "                acc_sr = accuracy_score(true_sr.flatten(), pred_sr.flatten())\n",
    "                acc_tr1 = accuracy_score(true_tr1.flatten(), pred_tr1.flatten())\n",
    "                acc_tr2 = accuracy_score(true_tr2.flatten(), pred_tr2.flatten())\n",
    "\n",
    "                plot_prediction(batch_img_sr, rec_sr_, batch_label_sr, cl_sr_, batch_img_tr1, rec_tr1_, batch_label_tr1, cl_tr1_,\n",
    "                                batch_img_tr2, rec_tr2_, batch_label_tr2, cl_tr2_, vmin = -1, vmax = 1, index = index_plt)\n",
    "                #print('prediction source domain sh/ex: ', np.argmax(dom_sh_sr_[index_plt]), np.argmax(dom_ex_sr_[index_plt]))\n",
    "                #print('prediction target domain sh/ex: ', np.argmax(dom_sh_tr_[index_plt]), np.argmax(dom_ex_tr_[index_plt]))\n",
    "                print('prediction source domain sh/ex: ', dom_sh_sr_[index_plt], dom_ex_sr_[index_plt])\n",
    "                print('prediction target1 domain sh/ex: ', dom_sh_tr1_[index_plt], dom_ex_tr1_[index_plt])\n",
    "                print('prediction target2 domain sh/ex: ', dom_sh_tr2_[index_plt], dom_ex_tr2_[index_plt])\n",
    "                print('prediction acc sr {:.2f}'.format(acc_sr))\n",
    "                print('prediction acc tr1 {:.2f}'.format(acc_tr1))\n",
    "                print('prediction acc tr2 {:.2f}'.format(acc_tr2))\n",
    "\n",
    "        # Evaluating model on validation,\n",
    "        val_dec_loss, val_dis_loss, val_sh_loss, val_ex_loss, val_ex_dis_loss, val_sh_dis_loss, val_cl_loss = [], [], [], [], [], [], []\n",
    "        for _ in range(0, num_of_val_batches):\n",
    "            #batch_t0, batch_t1 = next(valid_gen)\n",
    "            batch_img, batch_mask, batch_dom = next(valid_gen)\n",
    "            batch_img_sr = batch_img[:,:,:,:,0]\n",
    "            batch_img_tr1 = batch_img[:,:,:,:,1]\n",
    "            batch_img_tr2 = batch_img[:,:,:,:,2]\n",
    "            batch_label_sr = batch_mask[:,:,:,:,0]\n",
    "            batch_label_tr1 = batch_mask[:,:,:,:,1]\n",
    "            batch_label_tr2 = batch_mask[:,:,:,:,2]\n",
    "            batch_dom_sr = batch_dom[:,:,0]\n",
    "            batch_dom_tr1 = batch_dom[:,:,1]\n",
    "            batch_dom_tr2 = batch_dom[:,:,2]\n",
    "\n",
    "            if batch_img_sr.shape[0] != batch_size or batch_img_tr1.shape[0] != batch_size or batch_img_tr2.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            feed_dict = {img_sr: batch_img_sr, img_tr1: batch_img_tr1, img_tr2: batch_img_tr2,\n",
    "                        domain_label_sr: batch_dom_sr, domain_label_tr1:batch_dom_tr1, domain_label_tr2:batch_dom_tr2,\n",
    "                        class_label_sr: batch_label_sr}\n",
    "\n",
    "            with sess.as_default():\n",
    "                v_sh_loss = loss_sh.eval(feed_dict)\n",
    "                val_sh_loss.append(v_sh_loss)\n",
    "                v_ex_loss = loss_ex.eval(feed_dict)\n",
    "                val_ex_loss.append(v_ex_loss)\n",
    "                v_cl_loss = loss_cl.eval(feed_dict)\n",
    "                val_cl_loss.append(v_cl_loss)\n",
    "                v_dec_loss = loss_dec.eval(feed_dict)\n",
    "                val_dec_loss.append(v_dec_loss)\n",
    "                v_dis_loss = loss_dis.eval(feed_dict)\n",
    "                val_dis_loss.append(v_dis_loss)\n",
    "                v_ex_dis_loss = loss_feat_ex_dom_dis.eval(feed_dict)\n",
    "                val_ex_dis_loss.append(v_ex_dis_loss)\n",
    "                v_sh_dis_loss = loss_feat_sh_dom_dis.eval(feed_dict)\n",
    "                val_sh_dis_loss.append(v_sh_dis_loss)\n",
    "\n",
    "        if best_val_loss > np.mean(val_cl_loss):\n",
    "            patience = 10\n",
    "            best_val_loss = np.mean(val_cl_loss)\n",
    "            print('Saving best model and checkpoints') \n",
    "            save_model(encoder_sh, path_model_tm +'/'+'enc_sh.h5')\n",
    "            save_model(encoder_ex, path_model_tm +'/'+'enc_ex.h5')\n",
    "            save_model(decoder_sh, path_model_tm +'/'+'dec_sh.h5')\n",
    "            save_model(classifier, path_model_tm +'/'+'classifer.h5')\n",
    "            save_model(domain_discriminator, path_model_tm +'/'+'dis.h5')\n",
    "            print('Ok')\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience < 0:\n",
    "            print('[***] end training ...')\n",
    "            break\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('loss dec tr {:.2f}'.format(np.mean(dec_loss)), 'loss dec val {:.2f}'.format(np.mean(val_dec_loss)))\n",
    "        print('loss dis tr {:.2f}'.format(np.mean(dis_loss)), 'loss dis val {:.2f}'.format(np.mean(val_dis_loss)))\n",
    "        print('loss sh tr {:.2f}'.format(np.mean(enc_sh_loss)), 'loss sh val {:.2f}'.format(np.mean(val_sh_loss)))\n",
    "        print('loss ex tr {:.2f}'.format(np.mean(enc_ex_loss)), 'loss ex val {:.2f}'.format(np.mean(val_ex_loss)))\n",
    "        print('loss cl tr {:.2f}'.format(np.mean(cl_loss)), 'loss cl val {:.2f}'.format(np.mean(val_cl_loss)))\n",
    "        print('loss sh dis tr {:.2f}'.format(np.mean(sh_dis_loss)), 'loss sh dis val {:.2f}'.format(np.mean(val_sh_dis_loss)))\n",
    "        print('loss ex dis tr {:.2f}'.format(np.mean(ex_dis_loss)), 'loss ex dis val {:.2f}'.format(np.mean(val_ex_dis_loss)))\n",
    "        # save training loss\n",
    "        tr_dec.append(np.mean(dec_loss))\n",
    "        tr_dis.append(np.mean(dis_loss))\n",
    "        tr_esh.append(np.mean(enc_sh_loss))\n",
    "        tr_eex.append(np.mean(enc_ex_loss))\n",
    "        tr_cls.append(np.mean(cl_loss))\n",
    "        tr_dsh.append(np.mean(sh_dis_loss))\n",
    "        tr_dex.append(np.mean(ex_dis_loss))\n",
    "        # save validation loss\n",
    "        vl_dec.append(np.mean(val_dec_loss))\n",
    "        vl_dis.append(np.mean(val_dis_loss))\n",
    "        vl_esh.append(np.mean(val_sh_loss))\n",
    "        vl_eex.append(np.mean(val_ex_loss))\n",
    "        vl_cls.append(np.mean(val_cl_loss))\n",
    "        vl_dsh.append(np.mean(val_sh_dis_loss))\n",
    "        vl_dex.append(np.mean(val_ex_dis_loss))    \n",
    "        np.savetxt(path_loss_tm + '/training_loss.txt',\n",
    "                   [\"Decoder: %s\" % np.asarray(tr_dec), \"Discriminador: %s\" % np.asarray(tr_dis),\n",
    "                    \"EncSH: %s\" % np.asarray(tr_esh), \"EncEX: %s\" % np.asarray(tr_eex), \"Classifier: %s\" % np.asarray(tr_cls),\n",
    "                    \"DisSH: %s\" % np.asarray(tr_dsh), \"DisEX: %s\" % np.asarray(tr_dex),], fmt='%s', delimiter='\\n')\n",
    "        np.savetxt(path_loss_tm + '/validation_loss.txt',\n",
    "                   [\"Decoder: %s\" % np.asarray(vl_dec), \"Discriminador: %s\" % np.asarray(vl_dis),\n",
    "                    \"EncSH: %s\" % np.asarray(vl_esh), \"EncEX: %s\" % np.asarray(vl_eex), \"Classifier: %s\" % np.asarray(vl_cls),\n",
    "                    \"DisSH: %s\" % np.asarray(vl_dsh), \"DisEX: %s\" % np.asarray(vl_dex),], fmt='%s', delimiter='\\n')\n",
    "    tr_time.append(elapsed_time)\n",
    "    \n",
    "tr_time_ = np.asarray(tr_time)\n",
    "np.save(path_exp+'/tr_times.npy', tr_time_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference - tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pool = 4\n",
    "n_rows = 5\n",
    "n_cols = 5\n",
    "output_c_dim = 3\n",
    "#overlap_percent = 0.4\n",
    "\n",
    "im_test_dir = dir_data + test_name+'_1C/'\n",
    "tr_img = np.load(im_test_dir + test_name+'_1C_RGB_img_filt_norm_2020_2021_10B.npy')\n",
    "tr_row = tr_img.shape[0]\n",
    "tr_col = tr_img.shape[1]\n",
    "print('Test on: ', test_name+'_1C_RGB_img_filt_norm_2020_2021')\n",
    "print('min, max values: ', np.min(tr_img), np.max(tr_img))\n",
    "ref = np.load(im_test_dir + test_name+'_1C_ref_2020_2021.npy')\n",
    "\n",
    "# new size\n",
    "patch_size_rows, patch_size_cols = new_shape_tiles (tr_img, n_pool, n_rows, n_cols)\n",
    "print(patch_size_rows, patch_size_cols)\n",
    "nb_filters_sh = 16\n",
    "nb_filters_c = 16\n",
    "\n",
    "if save_prob == False:\n",
    "    prob_rec = np.zeros((tr_img.shape[0],tr_img.shape[1], times), dtype = np.float32)\n",
    "\n",
    "ts_time = []\n",
    "for tm in range(0, times):\n",
    "    print('time: ', tm)\n",
    "    \n",
    "    path_model_tm = path_models + '/run_'+str(tm)\n",
    "\n",
    "    model_enc_sh = load_model(path_model_tm  + '/' + 'enc_sh.h5', compile=False,\n",
    "                              custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "    model_classifier = load_model(path_model_tm + '/' + 'classifer.h5', compile=False)\n",
    "\n",
    "    c = tr_img.shape[-1]\n",
    "    print('shapes: ', tr_img.shape, ref.shape)\n",
    "    new_model_enc_sh = build_encoder_sh((patch_size_rows,patch_size_cols, c), nb_filters=nb_filters_sh, name='enc_sh_new_')\n",
    "    new_sh_out_shape = new_model_enc_sh.layers[-1].output_shape[1:]\n",
    "    print('encoder sh output shape: ', new_sh_out_shape)\n",
    "    new_model_classifier = build_classifier((new_sh_out_shape), nb_filters= nb_filters_c, num_classes = 3, name='cl_new_')\n",
    "    start_test = time.time()\n",
    "    prob = inference_classifier_tiles(tr_img, n_pool, n_rows, n_cols, output_c_dim, \n",
    "                                      model_enc_sh, new_model_enc_sh, model_classifier, new_model_classifier)\n",
    "    elapsed_time = time.time() - start_test\n",
    "    ts_time.append(elapsed_time)\n",
    "    \n",
    "    prob = prob[:tr_img.shape[0], :tr_img.shape[1]]\n",
    "    print('prob range: ', np.min(prob), np.max(prob), prob.shape)\n",
    "    #plt.imshow(prob)\n",
    "    \n",
    "    if save_prob == True:\n",
    "        np.save(path_maps+'/'+'prob_map_'+str(tm)+'.npy',prob) \n",
    "    \n",
    "    if save_prob == False:\n",
    "        prob_rec[:,:,tm] = prob\n",
    "        \n",
    "    del prob, new_model_enc_sh, new_model_classifier\n",
    "ts_time_ = np.asarray(ts_time)\n",
    "np.save(path_exp+'/ts_times.npy', ts_time_)\n",
    "del tr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_prob == True:\n",
    "    prob_rec = np.zeros((tr_row, tr_col, times), dtype = np.float32)\n",
    "    for tm in range (0, times):\n",
    "        print(tm)\n",
    "        prob_rec[:,:,tm] = np.load(path_maps+'/'+'prob_map_'+str(tm)+'.npy').astype(np.float32)\n",
    "\n",
    "mean_prob = np.mean(prob_rec, axis = -1)\n",
    "\n",
    "print('[*] min-max values... ', np.min(mean_prob), np.max(mean_prob))\n",
    "np.save(path_maps+'/prob_mean_'+ list_dir[0][:2] + '_2_'+ test_name +'.npy', mean_prob)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mean_prob, cmap ='jet')\n",
    "plt.axis('off')\n",
    "plt.savefig(path_maps+ '/prob_class_def_tile_' + list_dir[0][:2] + '_2_'+ test_name + '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test = np.load(im_test_dir +'test_mask_'+ test_name +'.npy')\n",
    "print('test mask: ', 'test_mask_'+ test_name +'.npy')\n",
    "ref = ref[:mask_test.shape[0], :mask_test.shape[1]]\n",
    "print('clases: ', np.unique(ref))\n",
    "prob_map = mean_prob[:mask_test.shape[0], :mask_test.shape[1]]\n",
    "\n",
    "ProbList = [0.5]\n",
    "if test_name == 'MA':\n",
    "    pixel_a = 100\n",
    "else:\n",
    "    pixel_a = 625\n",
    "    \n",
    "print('pixel area: ', pixel_a)\n",
    "    \n",
    "metrics_05 = matrics_AA_recall(ProbList, prob_map, ref, mask_test, pixel_a)\n",
    "print('recall, precision, f1-score: ', metrics_05*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1 = np.ones_like(ref).astype(np.float32)\n",
    "\n",
    "ref1 [ref == 2] = 0\n",
    "TileMask = mask_test * ref1\n",
    "GTTruePositives = ref == 1\n",
    "    \n",
    "Npoints = 30\n",
    "Pmax = np.max(prob_map[GTTruePositives * mask_test ==1])\n",
    "ProbList = np.linspace(Pmax,0,Npoints)\n",
    "    \n",
    "metrics_ = matrics_AA_recall(ProbList, prob_map, ref, mask_test, pixel_a)\n",
    "np.save(path_exp+'/acc_metrics_'+list_dir[0][:2] + '_2_'+test_name+'.npy',metrics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_copy = metrics_.copy()\n",
    "#metrics_copy[:4,1] = np.nan\n",
    "metrics_copy = complete_nan_values(metrics_copy)\n",
    "print(metrics_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_copy[0,1] = metrics_copy[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall = metrics_copy[:,0]\n",
    "Precision = metrics_copy[:,1]\n",
    "\n",
    "mAP = Area_under_the_curve(Recall, Precision)\n",
    "print('mAP: ', mAP)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(Recall, Precision)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference - patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability map\n",
    "prob_map = inference_classifier_patch(patch_size, overlap_percent, tr_img, output_c_dim, model_enc_sh, model_classifier)\n",
    "#np.save(path_maps + '/prob_map_patch.npy', prob_map[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,10))\n",
    "plt.imshow(prob_map[:,:,1], cmap = 'jet')\n",
    "plt.axis('off')   \n",
    "plt.savefig(path_maps+ '/prob_class_def_patch_' + test_name + '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clases: ', np.unique(ref))\n",
    "prob = prob_map[:,:,1]\n",
    "prob_map = prob[:mask_test.shape[0], :mask_test.shape[1]]\n",
    "ref1 = np.ones_like(ref).astype(np.float32)\n",
    "\n",
    "ref1 [ref == 2] = 0\n",
    "TileMask = mask_test * ref1\n",
    "GTTruePositives = ref==1\n",
    "    \n",
    "#Npoints = 20\n",
    "#Pmax = np.max(prob_map[GTTruePositives * TileMask ==1])\n",
    "#ProbList = np.linspace(Pmax,0,Npoints)\n",
    "ProbList = [0.5]\n",
    "    \n",
    "metrics_05 = matrics_AA_recall(ProbList, prob_map, ref, mask_test, 625)\n",
    "print('recall, precision, f1-score: ', metrics_05*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(path_maps + '/prob_map_patch.npy', prob_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***********"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE_sep_opt_Discriminator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
